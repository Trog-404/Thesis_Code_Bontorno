{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This Python script is used to extract and save metadata from all TIFF image files in a specified Google Drive folder into a structured JSON file. It is designed to run in Google Colab."
      ],
      "metadata": {
        "id": "si5tTyve7TAe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YOdvZhe7R5Z"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import tifffile\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Mount Google Drive to access files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define input and output folders\n",
        "folder_path = '/content/drive/My Drive/EOD_2'\n",
        "output_folder = '/content/drive/My Drive/EOD_2/'\n",
        "os.makedirs(output_folder, exist_ok=True)  # Ensure output folder exists\n",
        "\n",
        "# Dictionary to store metadata for all images\n",
        "all_metadata = {}\n",
        "\n",
        "# Iterate through each file in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "    # Process only TIFF files\n",
        "    if filename.endswith('.tif') or filename.endswith('.tiff'):\n",
        "        image_path = os.path.join(folder_path, filename)\n",
        "        metadata = {}\n",
        "\n",
        "        # Open the TIFF image and extract metadata\n",
        "        with tifffile.TiffFile(image_path) as sem_image:\n",
        "            for page in sem_image.pages:\n",
        "                for tag in page.tags.values():\n",
        "                    # Convert NumPy arrays to lists for JSON serialization\n",
        "                    if isinstance(tag.value, np.ndarray):\n",
        "                        metadata[tag.name] = tag.value.tolist()\n",
        "                    else:\n",
        "                        metadata[tag.name] = tag.value\n",
        "\n",
        "        # Store metadata using filename as the key\n",
        "        all_metadata[filename] = metadata\n",
        "        print(f\"Metadata for {filename} added to the collection\")\n",
        "\n",
        "# Save all metadata as a JSON file\n",
        "output_filename = 'all_metadata10.json'\n",
        "output_path = os.path.join(output_folder, output_filename)\n",
        "with open(output_path, 'w') as f:\n",
        "    json.dump(all_metadata, f, indent=4)\n",
        "\n",
        "print(f\"All metadata saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script bellow extracts specific SEM image metadata from a JSON file and writes it into a new filtered JSON file."
      ],
      "metadata": {
        "id": "HRUsO-r37f0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "# Mount Google Drive to access files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the input metadata JSON file\n",
        "file_path = '/content/drive/My Drive/EOD_2_SN/SN/all_metadata.json'\n",
        "\n",
        "# Load the JSON data from the file\n",
        "with open(file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Define the metadata keys to extract\n",
        "keys_to_extract = [\n",
        "    \"image_name\", \"dp_detector_type\", \"ap_actualkv\",\n",
        "    \"ap_wd\", \"ap_aperturesize\"\n",
        "]\n",
        "\n",
        "# List to hold the filtered metadata\n",
        "extracted_data = []\n",
        "\n",
        "# Iterate through each image in the dataset\n",
        "for image_name, image_data in data.items():\n",
        "    # Access the nested \"CZ_SEM\" dictionary\n",
        "    cz_sem_data = image_data.get(\"CZ_SEM\", {})\n",
        "\n",
        "    if cz_sem_data:\n",
        "        # Dictionary to store extracted fields for the current image\n",
        "        extracted_item = {}\n",
        "\n",
        "        # Add the image name explicitly\n",
        "        extracted_item['image_name'] = image_name\n",
        "\n",
        "        # Extract each requested key from the CZ_SEM block\n",
        "        for key in keys_to_extract:\n",
        "            extracted_item[key] = cz_sem_data.get(key)\n",
        "\n",
        "        # Append the extracted record to the results list\n",
        "        extracted_data.append(extracted_item)\n",
        "\n",
        "# Define output path for the filtered JSON data\n",
        "output_file_path = '/content/drive/My Drive/EOD_2_SN/SN/filtered_data_all2.json'\n",
        "\n",
        "# Save the extracted metadata to the output file in JSON format\n",
        "with open(output_file_path, 'w') as outfile:\n",
        "    json.dump(extracted_data, outfile, indent=4)\n",
        "\n",
        "print(f\"Extracted data written to: {output_file_path}\")"
      ],
      "metadata": {
        "id": "tGfntyvS7h8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script bellow designed to generate and save a full-factorial experimental design table for Scanning Electron Microscopy (SEM) parameter testing."
      ],
      "metadata": {
        "id": "LsFjQRY2PtMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from itertools import product\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive to access files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define levels for SEM parameters\n",
        "working_distance = [3, 5, 7, 10]     # in millimeters (mm)\n",
        "EHT = [1.5, 3, 5, 10]               # accelerating voltage in kilovolts (kV)\n",
        "aperture_size = [10, 20, 30, 60]    # in micrometers (µm)\n",
        "\n",
        "# Generate all possible combinations of the parameter values (full factorial design)\n",
        "factorial_design = list(product(working_distance, EHT, aperture_size))\n",
        "\n",
        "# Create a DataFrame to represent the experimental design\n",
        "experiment_table = pd.DataFrame(\n",
        "    factorial_design,\n",
        "    columns=['Working Distance (mm)', 'EHT (kV)', 'Aperture Size (µm)']\n",
        ")\n",
        "\n",
        "# Insert a sequential run number as the first column\n",
        "experiment_table.insert(0, 'Run #', range(1, len(experiment_table) + 1))\n",
        "\n",
        "# Display the DataFrame in Colab's output\n",
        "print(experiment_table)\n",
        "\n",
        "# Save the design to a CSV file in Google Drive\n",
        "experiment_table.to_csv(\n",
        "    '/content/drive/My Drive/EOD_2/full_factorial_design_10.csv',\n",
        "    index=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "bHGqTxLZPteH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script automatically renames and organizes SEM image files based on metadata embedded in their TIFF headers."
      ],
      "metadata": {
        "id": "YP3hByKqQE6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import re\n",
        "import tifffile\n",
        "from datetime import datetime\n",
        "from shutil import copyfile\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define input and output folders in Google Drive\n",
        "input_folder = '/content/drive/My Drive/EOD_2_SN/EOD-2/'\n",
        "output_folder = '/content/drive/My Drive/EOD_2_SN/SN/'\n",
        "\n",
        "# Create output folder if it doesn't exist\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "\n",
        "def clean(text):\n",
        "    \"\"\"\n",
        "    Sanitize strings for use in filenames:\n",
        "    replaces non-alphanumeric characters with underscores.\n",
        "    \"\"\"\n",
        "    return re.sub(r'[^\\w\\-]', '_', str(text)).strip()\n",
        "\n",
        "\n",
        "# Index for generating sequential filenames\n",
        "index = 1\n",
        "\n",
        "# Loop through sorted list of files in the input folder\n",
        "for filename in sorted(os.listdir(input_folder)):\n",
        "    # Skip non-TIFF files\n",
        "    if not filename.lower().endswith('.tif'):\n",
        "        continue\n",
        "\n",
        "    # Full path to the original image\n",
        "    original_path = os.path.join(input_folder, filename)\n",
        "\n",
        "    try:\n",
        "        # Open the TIFF file and access the CZ_SEM metadata tag\n",
        "        with tifffile.TiffFile(original_path) as tif:\n",
        "            cz_sem = tif.pages[0].tags.get('CZ_SEM')\n",
        "\n",
        "            # Skip if no CZ_SEM metadata is found\n",
        "            if cz_sem is None:\n",
        "                print(f\"⚠️ No CZ_SEM metadata in {filename}\")\n",
        "                continue\n",
        "\n",
        "            # Extract metadata dictionary\n",
        "            metadata = cz_sem.value if isinstance(\n",
        "                cz_sem.value, dict\n",
        "            ) else tif.pages[0].tags['CZ_SEM'].value\n",
        "\n",
        "        # Safely extract relevant metadata fields with fallbacks\n",
        "        dp_detector_type = clean(\n",
        "            metadata.get('dp_detector_type', ['Detector', 'UNKNOWN'])[1]\n",
        "        )\n",
        "\n",
        "        # Extract and format the acquisition date\n",
        "        ap_date_raw = metadata.get('ap_date', ['Date', 'UNKNOWN'])[1]\n",
        "        try:\n",
        "            ap_date = datetime.strptime(\n",
        "                ap_date_raw, \"%d %b %Y\"\n",
        "            ).strftime(\"%Y%m%d\")\n",
        "        except Exception:\n",
        "            ap_date = \"UNKNOWNDATE\"\n",
        "\n",
        "        # Extract and clean the sample name\n",
        "        sv_file_name = clean(\n",
        "            metadata.get('sv_file_name', ['File Name', filename])[1].replace('.tif', '')\n",
        "        )\n",
        "\n",
        "        # Extract and format the magnification\n",
        "        ap_mag_val = metadata.get('ap_mag', ['Mag', ''])[1]\n",
        "        ap_mag = (\n",
        "            f\"{ap_mag_val}X\"\n",
        "            if isinstance(ap_mag_val, (int, float))\n",
        "            else clean(str(ap_mag_val))\n",
        "        )\n",
        "\n",
        "        # Generate new standardized filename\n",
        "        new_name = f\"{dp_detector_type}_{ap_date}_{sv_file_name}_{ap_mag}_{index:03d}.tif\"\n",
        "        new_path = os.path.join(output_folder, new_name)\n",
        "\n",
        "        # Copy the original image to the new location with the new name\n",
        "        copyfile(original_path, new_path)\n",
        "\n",
        "        print(f\"✅ Copied: {filename} → {new_name}\")\n",
        "        index += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        # Log any errors during processing\n",
        "        print(f\"❌ Error with {filename}: {e}\")\n"
      ],
      "metadata": {
        "id": "5cpX-H6lQFEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script bellow is designed to evaluate the visual quality of SEM (Scanning Electron Microscopy) images using a no-reference metric called BRISQUE, and to extract associated image metadata embedded within the TIFF files."
      ],
      "metadata": {
        "id": "qZuDklJsQq0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access SEM image files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the folder path containing SEM .tif images\n",
        "folder_path = '/content/drive/My Drive/EOD_2_SN/SN/'\n",
        "\n",
        "# Install required libraries for image processing and BRISQUE computation\n",
        "!pip install piq torch torchvision pillow pandas\n",
        "\n",
        "# Import necessary Python libraries\n",
        "import torch\n",
        "import os\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import piq\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import tifffile as tiff\n",
        "\n",
        "\n",
        "def make_metadata_with_brisque_score_tiff(folder_path, mode):\n",
        "    \"\"\"\n",
        "    Compute BRISQUE quality scores for a batch of TIFF SEM images\n",
        "    and extract selected metadata from each image. Save the combined\n",
        "    results to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        folder_path (str): Path to the folder containing TIFF files.\n",
        "        mode (str): Write mode ('w' for write, 'a' for append).\n",
        "    \"\"\"\n",
        "    scores = []           # List to store BRISQUE scores\n",
        "    metadata_list = []    # List to store extracted metadata\n",
        "\n",
        "    # Get all .tif files in the folder\n",
        "    files = glob(folder_path + '*.tif')\n",
        "\n",
        "    # Metadata fields to retain\n",
        "    keys_to_keep = [\n",
        "        'image_name', 'ap_aperturesize', 'ap_wd',\n",
        "        'sv_file_name', 'ap_actualkv'\n",
        "    ]\n",
        "\n",
        "    # Iterate over each image\n",
        "    for img_name in files:\n",
        "        print(f\"Processing: {img_name}\")\n",
        "        image_path = img_name\n",
        "\n",
        "        # Open image and convert to RGB\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        width, height = image.size\n",
        "\n",
        "        # Crop image: keep full width but limit height to 691 pixels\n",
        "        cropped_image = image.crop((0, 0, width, min(691, height)))\n",
        "\n",
        "        # Convert image to tensor (shape: [1, C, H, W])\n",
        "        to_tensor = T.ToTensor()\n",
        "        img_tensor = to_tensor(cropped_image).unsqueeze(0)\n",
        "\n",
        "        # Compute BRISQUE score using PIQ\n",
        "        score = piq.brisque(img_tensor, data_range=1.0)\n",
        "        scores.append(score.item())\n",
        "\n",
        "        # Extract CZ_SEM metadata from TIFF tags\n",
        "        with tiff.TiffFile(image_path) as tif:\n",
        "            cz_sem_metadata = None\n",
        "\n",
        "            # Search for CZ_SEM metadata tag\n",
        "            for page in tif.pages:\n",
        "                for tag in page.tags.values():\n",
        "                    if tag.name == 'CZ_SEM':\n",
        "                        cz_sem_metadata = tag.value\n",
        "                        break\n",
        "                if cz_sem_metadata:\n",
        "                    break\n",
        "\n",
        "            # Extract and clean metadata\n",
        "            parsed_metadata = {}\n",
        "            if cz_sem_metadata and isinstance(cz_sem_metadata, dict):\n",
        "                for key in keys_to_keep:\n",
        "                    value = cz_sem_metadata.get(key, None)\n",
        "                    # Extract only value if it's a tuple\n",
        "                    if isinstance(value, tuple) and len(value) > 0:\n",
        "                        parsed_metadata[key] = value[1]\n",
        "                    else:\n",
        "                        parsed_metadata[key] = value\n",
        "            else:\n",
        "                # Fill with None if no metadata found\n",
        "                parsed_metadata = {key: None for key in keys_to_keep}\n",
        "\n",
        "            metadata_list.append(parsed_metadata)\n",
        "\n",
        "    # Convert metadata and scores to DataFrames\n",
        "    metadata_df = pd.DataFrame(metadata_list)\n",
        "    score_df = pd.DataFrame({'File': files, 'Score': scores})\n",
        "\n",
        "    # Combine scores and metadata into one table\n",
        "    result_df = pd.concat([score_df, metadata_df], axis=1)\n",
        "\n",
        "    # Write header only when mode is not append\n",
        "    header = mode != 'a'\n",
        "\n",
        "    # Save the result to CSV\n",
        "    result_df.to_csv(\n",
        "        os.path.join(folder_path, 'brisque_scores_with_metadata10.csv'),\n",
        "        index=False,\n",
        "        header=header,\n",
        "        mode=mode\n",
        "    )\n",
        "\n",
        "\n",
        "# Run the function to process images and export BRISQUE scores\n",
        "make_metadata_with_brisque_score_tiff(folder_path, mode='w')\n"
      ],
      "metadata": {
        "id": "e9zPNm2hQrIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scripts below automates the process of quality assessment and metadata extraction for Scanning Electron Microscopy (SEM) images saved in TIFF format. It computes a no-reference image quality score using the NIQE (Natural Image Quality Evaluator) metric and retrieves important metadata from each image’s CZ_SEM tag.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DfzaVbVqSVum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import tifffile as tiff\n",
        "from skimage import io, img_as_float\n",
        "import pyiqa\n",
        "\n",
        "\n",
        "# Load NIQE model from pyiqa for no-reference image quality assessment\n",
        "niqe_model = pyiqa.create_metric('niqe')\n",
        "\n",
        "\n",
        "def make_metadata_with_niqe_score_tiff(folder_path, mode):\n",
        "    \"\"\"\n",
        "    Computes NIQE scores for TIFF SEM images and extracts associated metadata.\n",
        "\n",
        "    Parameters:\n",
        "        folder_path (str): Path to the folder containing .tif SEM images.\n",
        "        mode (str): 'w' to write a new CSV, 'a' to append to existing CSV.\n",
        "\n",
        "    Output:\n",
        "        Saves a CSV file named 'niqe_scores_with_metadata.csv' to the same folder.\n",
        "    \"\"\"\n",
        "    scores = []  # Stores NIQE scores\n",
        "    metadata_list = []  # Stores parsed TIFF metadata\n",
        "\n",
        "    # Get all .tif files in the specified folder\n",
        "    files = glob(os.path.join(folder_path, '*.tif'))\n",
        "\n",
        "    # Define the metadata fields to extract from CZ_SEM tag\n",
        "    keys_to_keep = ['image_name', 'ap_aperturesize', 'ap_wd', 'sv_file_name', 'ap_actualkv']\n",
        "\n",
        "    # Iterate through each image file\n",
        "    for img_path in files:\n",
        "        print(f\"Processing: {img_path}\")\n",
        "\n",
        "        try:\n",
        "            # Load image using skimage, convert to grayscale if RGB, normalize\n",
        "            image = io.imread(img_path)\n",
        "            if image.ndim == 3:  # Convert RGB to grayscale\n",
        "                image = image[:, :, 0]\n",
        "            image = img_as_float(image)  # Convert to float [0,1]\n",
        "            image_tensor = torch.tensor(image).unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, H, W]\n",
        "\n",
        "            # Calculate NIQE score and store it\n",
        "            score = niqe_model(image_tensor).item()\n",
        "            scores.append(score)\n",
        "\n",
        "            print(f\"{os.path.basename(img_path)}: NIQE = {score:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {img_path}: {e}\")\n",
        "            scores.append(None)\n",
        "\n",
        "        # --- Extract Metadata from TIFF tags ---\n",
        "        with tiff.TiffFile(img_path) as tif:\n",
        "            cz_sem_metadata = None\n",
        "\n",
        "            # Look for 'CZ_SEM' tag in TIFF pages\n",
        "            for page in tif.pages:\n",
        "                for tag in page.tags.values():\n",
        "                    if tag.name == 'CZ_SEM':\n",
        "                        cz_sem_metadata = tag.value\n",
        "                        break\n",
        "                if cz_sem_metadata:\n",
        "                    break\n",
        "\n",
        "            # Parse metadata fields\n",
        "            parsed_metadata = {}\n",
        "            if cz_sem_metadata and isinstance(cz_sem_metadata, dict):\n",
        "                for key in keys_to_keep:\n",
        "                    value = cz_sem_metadata.get(key, None)\n",
        "                    parsed_metadata[key] = value[1] if isinstance(value, tuple) else value\n",
        "            else:\n",
        "                parsed_metadata = {key: None for key in keys_to_keep}\n",
        "\n",
        "            metadata_list.append(parsed_metadata)\n",
        "\n",
        "    # --- Build DataFrame and Save Results ---\n",
        "\n",
        "    # Create DataFrames from scores and metadata\n",
        "    metadata_df = pd.DataFrame(metadata_list)\n",
        "    scores_df = pd.DataFrame({\n",
        "        'File': files,\n",
        "        'Score': scores\n",
        "    })\n",
        "\n",
        "    # Merge and write to CSV\n",
        "    result_df = pd.concat([scores_df, metadata_df], axis=1)\n",
        "    output_csv_path = os.path.join(folder_path, 'niqe_scores_with_metadata.csv')\n",
        "\n",
        "    result_df.to_csv(\n",
        "        output_csv_path,\n",
        "        index=False,\n",
        "        mode=mode,\n",
        "        header=(mode != 'a')\n",
        "    )\n",
        "\n",
        "    print(f\"\\n✅ NIQE scores saved to: {output_csv_path}\")\n",
        "\n",
        "\n",
        "# Run the function\n",
        "make_metadata_with_niqe_score_tiff('/content/drive/My Drive/EOD_2_SN/SN/', mode='w')\n"
      ],
      "metadata": {
        "id": "5dboZFABSV4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script merges two CSV files—one containing BRISQUE scores and the other containing NIQE scores—for a set of SEM .tif images. It matches the records by image filename (File), combines quality scores and key SEM metadata (e.g., aperture size, working distance, voltage), and saves the result as a single, clean CSV file."
      ],
      "metadata": {
        "id": "z02LipLvUZdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 🚀 Mount Google Drive to access files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 📁 Define the folder path where the CSV files are stored\n",
        "folder_path = '/content/drive/My Drive/EOD_2_SN/SN/'\n",
        "\n",
        "# 📄 Define full paths to the BRISQUE and NIQE CSV files\n",
        "brisque_path = os.path.join(folder_path, 'brisque_scores_with_metadata.csv')\n",
        "niqe_path = os.path.join(folder_path, 'niqe_scores_with_metadata.csv')\n",
        "\n",
        "# 📥 Load both CSV files into pandas DataFrames\n",
        "brisque_df = pd.read_csv(brisque_path)\n",
        "niqe_df = pd.read_csv(niqe_path)\n",
        "\n",
        "# 🔗 Merge the DataFrames on the 'File' column\n",
        "# Keep 'Score' from NIQE and rename overlapping columns\n",
        "merged_df = pd.merge(\n",
        "    brisque_df,\n",
        "    niqe_df[['File', 'Score']],\n",
        "    on='File',\n",
        "    suffixes=('_BRISQUE', '_NIQE')\n",
        ")\n",
        "\n",
        "# 🏷️ Rename and clean up column names for clarity\n",
        "merged_df = merged_df.rename(columns={\n",
        "    'File': 'Image_File',\n",
        "    'Score_BRISQUE': 'BRISQUE_Score',\n",
        "    'Score_NIQE': 'NIQE_Score',\n",
        "    'ap_aperturesize': 'Aperture_Size',\n",
        "    'ap_wd': 'Working_Distance',\n",
        "    'ap_actualkv': 'Accelerating_Voltage',\n",
        "    'sv_file_name': 'Sample_Name'\n",
        "})\n",
        "\n",
        "# 🎯 Select the final columns to keep in the output\n",
        "final_df = merged_df[[\n",
        "    'Image_File', 'Sample_Name', 'Accelerating_Voltage',\n",
        "    'Working_Distance', 'Aperture_Size',\n",
        "    'BRISQUE_Score', 'NIQE_Score'\n",
        "]]\n",
        "\n",
        "# 💾 Save the combined dataset to a new CSV file\n",
        "output_path = os.path.join(folder_path, 'combined_sem_dataset.csv')\n",
        "final_df.to_csv(output_path, index=False)\n",
        "\n",
        "# ✅ Confirmation message\n",
        "print(f\"✅ Combined dataset saved to: {output_path}\")\n"
      ],
      "metadata": {
        "id": "2A5r6e7EUZol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code analyzes and clusters SEM image quality data based on BRISQUE scores using K-Means clustering, and evaluates the importance of imaging parameters (aperture size, working distance, and accelerating voltage) using a Random Forest regressor. The goal is to uncover patterns in image quality and identify which parameters most influence it."
      ],
      "metadata": {
        "id": "sfY5E4b-Unwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📦 Import necessary libraries\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# 🚀 Mount Google Drive to access dataset\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 📄 Load the dataset containing BRISQUE scores and SEM metadata\n",
        "data = pd.read_csv('/content/drive/MyDrive/EOD_2_SN/SN/brisque_scores_with_metadata.csv')\n",
        "\n",
        "# 🏷️ Rename columns for simplicity and consistency\n",
        "data = data.rename(columns={\n",
        "    'ap_aperturesize': 'Aperture_Size',\n",
        "    'ap_wd': 'Working_Distance',\n",
        "    'ap_actualkv': 'Accelerating_Voltage',\n",
        "    'Score': 'BRISQUE_Score'\n",
        "})\n",
        "\n",
        "# 🎯 Select relevant numerical features for clustering and analysis\n",
        "data_cleaned = data[[\n",
        "    'Aperture_Size', 'Working_Distance',\n",
        "    'Accelerating_Voltage', 'BRISQUE_Score'\n",
        "]]\n",
        "\n",
        "# ⚖️ Standardize the data for clustering (zero mean, unit variance)\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data_cleaned)\n",
        "\n",
        "# 📈 Use the elbow method to determine optimal number of clusters (k)\n",
        "inertia = []\n",
        "k_range = range(1, 11)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(data_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# 📊 Plot elbow curve to visualize optimal k\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_range, inertia, marker='o')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal k (BRISQUE)')\n",
        "plt.show()\n",
        "\n",
        "# 🔧 Set optimal number of clusters (as identified from elbow plot)\n",
        "optimal_k = 4\n",
        "\n",
        "# ⚙️ Perform K-Means clustering\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "clusters = kmeans.fit_predict(data_scaled)\n",
        "data['Cluster'] = clusters  # Add cluster labels to dataset\n",
        "\n",
        "# 🌲 Use Random Forest to determine feature importance for BRISQUE score\n",
        "X = data_cleaned.drop(columns=['BRISQUE_Score'])  # Features\n",
        "y = data_cleaned['BRISQUE_Score']  # Target\n",
        "\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# 🔍 Get feature importance values\n",
        "feature_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "\n",
        "# 🖨️ Print feature importance ranking\n",
        "print(\"Feature Importance Ranking (Random Forest - BRISQUE):\")\n",
        "print(feature_importances.sort_values(ascending=False))\n",
        "\n",
        "# 📉 2D visualization of clusters using BRISQUE Score and Working Distance\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(\n",
        "    x=data['BRISQUE_Score'],\n",
        "    y=data['Working_Distance'],\n",
        "    hue=data['Cluster'],\n",
        "    palette='viridis',\n",
        "    s=100\n",
        ")\n",
        "plt.xlabel('BRISQUE Score')\n",
        "plt.ylabel('Working Distance')\n",
        "plt.title('K-Means Clustering (BRISQUE Score vs Working Distance)')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n",
        "\n",
        "# 🔺 3D visualization using top 2 features + BRISQUE Score\n",
        "top_features = feature_importances.nlargest(2).index.tolist()\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "scatter = ax.scatter(\n",
        "    data[top_features[0]],\n",
        "    data[top_features[1]],\n",
        "    data['BRISQUE_Score'],\n",
        "    c=data['Cluster'],\n",
        "    cmap='viridis',\n",
        "    s=100\n",
        ")\n",
        "\n",
        "ax.set_xlabel(top_features[0])\n",
        "ax.set_ylabel(top_features[1])\n",
        "ax.set_zlabel('BRISQUE Score')\n",
        "ax.set_title('3D Cluster Visualization (Top Features - BRISQUE)')\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gMBQCTEgUn6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script analyzes and clusters SEM image quality data using K-Means clustering, based on NIQE scores. It also uses a Random Forest regressor to evaluate the importance of SEM imaging parameters (aperture size, working distance, and accelerating voltage) in determining image quality.\n",
        "\n"
      ],
      "metadata": {
        "id": "dBARx_kmVnnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📦 Import required libraries\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# 🚀 Mount Google Drive to access the dataset\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 📄 Load the CSV file containing NIQE scores and SEM metadata\n",
        "data = pd.read_csv('/content/drive/MyDrive/EOD_2_SN/SN/niqe_scores_with_metadata.csv')\n",
        "\n",
        "# 🏷️ Rename columns for clarity and consistency\n",
        "data = data.rename(columns={\n",
        "    'ap_aperturesize': 'Aperture_Size',\n",
        "    'ap_wd': 'Working_Distance',\n",
        "    'ap_actualkv': 'Accelerating_Voltage',\n",
        "    'Score': 'NIQE_Score'\n",
        "})\n",
        "\n",
        "# 🎯 Select only numeric features for clustering and modeling\n",
        "data_cleaned = data[[\n",
        "    'Aperture_Size',\n",
        "    'Working_Distance',\n",
        "    'Accelerating_Voltage',\n",
        "    'NIQE_Score'\n",
        "]]\n",
        "\n",
        "# ⚖️ Standardize the features to have zero mean and unit variance\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data_cleaned)\n",
        "\n",
        "# 📈 Use the Elbow Method to determine the optimal number of clusters (k)\n",
        "inertia = []\n",
        "k_range = range(1, 11)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(data_scaled)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# 📊 Plot the inertia to find the elbow point\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(k_range, inertia, marker='o')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal k (NIQE)')\n",
        "plt.show()\n",
        "\n",
        "# 🔧 Select the optimal number of clusters (e.g., from elbow plot)\n",
        "optimal_k = 4\n",
        "\n",
        "# ⚙️ Apply K-Means clustering with optimal k\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "clusters = kmeans.fit_predict(data_scaled)\n",
        "\n",
        "# 🧬 Add cluster labels to the original dataset\n",
        "data['Cluster'] = clusters\n",
        "\n",
        "# 🌲 Train a Random Forest to identify important features affecting NIQE score\n",
        "X = data_cleaned.drop(columns=['NIQE_Score'])  # Input features\n",
        "y = data_cleaned['NIQE_Score']  # Target variable\n",
        "\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# 🔍 Calculate feature importances\n",
        "feature_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "\n",
        "# 🖨️ Print feature ranking\n",
        "print(\"Feature Importance Ranking (Random Forest - NIQE):\")\n",
        "print(feature_importances.sort_values(ascending=False))\n",
        "\n",
        "# 📉 2D scatter plot to visualize clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(\n",
        "    x=data['NIQE_Score'],\n",
        "    y=data['Working_Distance'],\n",
        "    hue=data['Cluster'],\n",
        "    palette='viridis',\n",
        "    s=100\n",
        ")\n",
        "plt.xlabel('NIQE Score')\n",
        "plt.ylabel('Working Distance')\n",
        "plt.title('K-Means Clustering (NIQE Score vs Working Distance)')\n",
        "plt.legend(title='Cluster')\n",
        "plt.show()\n",
        "\n",
        "# 📈 3D scatter plot using top 2 features + NIQE score\n",
        "top_features = feature_importances.nlargest(2).index.tolist()\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "scatter = ax.scatter(\n",
        "    data[top_features[0]],\n",
        "    data[top_features[1]],\n",
        "    data['NIQE_Score'],\n",
        "    c=data['Cluster'],\n",
        "    cmap='viridis',\n",
        "    s=100\n",
        ")\n",
        "\n",
        "ax.set_xlabel(top_features[0])\n",
        "ax.set_ylabel(top_features[1])\n",
        "ax.set_zlabel('NIQE Score')\n",
        "ax.set_title('3D Cluster Visualization (Top Features - NIQE)')\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XLkXgYurVnxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script performs a 3D surface regression analysis on SEM image quality data, specifically using BRISQUE scores as the quality metric. It models the relationship between imaging parameters, aperture size, working distance, and accelerating voltage, and the resulting image quality using a second-degree polynomial function."
      ],
      "metadata": {
        "id": "yc8fyvmvWY0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📦 Import required libraries\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "# 🚀 Mount Google Drive to access files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 📁 Path to the BRISQUE metadata CSV\n",
        "file_path = '/content/drive/MyDrive/EOD_2_SN/SN/brisque_scores_with_metadata.csv'\n",
        "\n",
        "# 📌 Step 1: Load data from CSV\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 📌 Step 2: Extract independent and dependent variables\n",
        "x_data = df['ap_aperturesize'].values              # Aperture size (µm)\n",
        "y_data = df['ap_wd'].values                         # Working distance (mm)\n",
        "z_data = df['ap_actualkv'].values                  # Accelerating voltage (kV)\n",
        "score_data = df['Score'].values                    # BRISQUE image quality score (lower is better)\n",
        "\n",
        "# 📌 Step 3: Define a 3D second-degree polynomial regression surface\n",
        "def surface_function(X, a, b, c, d, e, f, g, h, i, j):\n",
        "    x, y, z = X\n",
        "    return (\n",
        "        a * x ** 2 + b * y ** 2 + c * z ** 2 +\n",
        "        d * x * y + e * x * z + f * y * z +\n",
        "        g * x + h * y + i * z + j\n",
        "    )\n",
        "\n",
        "# 📌 Step 4: Fit the model to the data\n",
        "params, _ = curve_fit(surface_function, (x_data, y_data, z_data), score_data)\n",
        "\n",
        "# Unpack coefficients for readability\n",
        "a, b, c, d, e, f, g, h, i, j = params\n",
        "\n",
        "# 🖨️ Show the model parameters\n",
        "print('🔹 Optimized Model Coefficients:')\n",
        "print(f'   a={a}, b={b}, c={c}, d={d}, e={e}, f={f}, g={g}, h={h}, i={i}, j={j}')\n",
        "\n",
        "# 📌 Step 5: Generate a mesh grid to evaluate the surface\n",
        "x_range = np.linspace(min(x_data), max(x_data), 30)\n",
        "y_range = np.linspace(min(y_data), max(y_data), 30)\n",
        "z_range = np.linspace(min(z_data), max(z_data), 30)\n",
        "\n",
        "X, Y, Z = np.meshgrid(x_range, y_range, z_range)\n",
        "Scores = surface_function((X, Y, Z), a, b, c, d, e, f, g, h, i, j)\n",
        "\n",
        "# 📌 Step 6: Find optimal conditions that minimize the BRISQUE score\n",
        "optimal_idx = np.unravel_index(np.argmin(Scores, axis=None), Scores.shape)\n",
        "optimal_x = X[optimal_idx]\n",
        "optimal_y = Y[optimal_idx]\n",
        "optimal_z = Z[optimal_idx]\n",
        "optimal_score = Scores[optimal_idx]\n",
        "\n",
        "# 🖨️ Display optimal SEM settings and lowest score\n",
        "print('✅ Optimal Conditions:')\n",
        "print(f'   ap_aperturesize = {optimal_x}')\n",
        "print(f'   ap_wd = {optimal_y}')\n",
        "print(f'   ap_actualkv = {optimal_z}')\n",
        "print(f'   Lowest Score (Best Quality) = {optimal_score}')\n",
        "\n",
        "# 📌 Step 7: Create a 3D surface plot (fixing one variable: accelerating voltage)\n",
        "z_fixed = np.mean(z_data)  # Fix accelerating voltage at its average\n",
        "X_plot, Y_plot = np.meshgrid(x_range, y_range)\n",
        "Z_plot = surface_function(\n",
        "    (X_plot, Y_plot, np.full_like(X_plot, z_fixed)),\n",
        "    a, b, c, d, e, f, g, h, i, j\n",
        ")\n",
        "\n",
        "# 📌 Step 8: Visualize the regression surface and data points\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot actual data points\n",
        "ax.scatter(x_data, y_data, score_data, color='red', label='Real Data')\n",
        "\n",
        "# Plot fitted surface\n",
        "ax.plot_surface(X_plot, Y_plot, Z_plot, cmap='viridis', alpha=0.7)\n",
        "\n",
        "# Set axis labels and title\n",
        "ax.set_xlabel('ap_aperturesize')\n",
        "ax.set_ylabel('ap_wd')\n",
        "ax.set_zlabel('Score')\n",
        "ax.set_title(f'Surface Regression (Fixed ap_actualkv = {z_fixed:.2f})')\n",
        "\n",
        "# Show legend and plot\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kn1d-s8SWY_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code fits a 3D polynomial regression model to predict NIQE image quality scores based on SEM imaging parameters: aperture size, working distance, and accelerating voltage. It finds the optimal parameter combination that minimizes the NIQE score and visualizes the regression surface along with actual data in a 3D plot."
      ],
      "metadata": {
        "id": "bIg2zS3QXWYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📦 Import required libraries\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "# 🚀 Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 📁 Path to the NIQE metadata CSV\n",
        "file_path = '/content/drive/MyDrive/EOD_2_SN/SN/niqe_scores_with_metadata.csv'\n",
        "\n",
        "# 📌 Step 1: Load data from CSV\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# 📌 Step 2: Extract variables\n",
        "x_data = df['ap_aperturesize'].values             # Aperture size (µm)\n",
        "y_data = df['ap_wd'].values                        # Working distance (mm)\n",
        "z_data = df['ap_actualkv'].values                 # Accelerating voltage (kV)\n",
        "score_data = df['Score'].values                   # NIQE image quality score\n",
        "\n",
        "# 📌 Step 3: Define a 3D second-degree polynomial regression surface\n",
        "def surface_function(X, a, b, c, d, e, f, g, h, i, j):\n",
        "    x, y, z = X\n",
        "    return (\n",
        "        a * x ** 2 + b * y ** 2 + c * z ** 2 +\n",
        "        d * x * y + e * x * z + f * y * z +\n",
        "        g * x + h * y + i * z + j\n",
        "    )\n",
        "\n",
        "# 📌 Step 4: Fit the polynomial surface to the data\n",
        "params, _ = curve_fit(surface_function, (x_data, y_data, z_data), score_data)\n",
        "\n",
        "# 📌 Step 5: Extract and print model coefficients\n",
        "a, b, c, d, e, f, g, h, i, j = params\n",
        "print('🔹 Optimized Model Coefficients:')\n",
        "print(f'   a={a}, b={b}, c={c}, d={d}, e={e}, f={f}, g={g}, h={h}, i={i}, j={j}')\n",
        "\n",
        "# 📌 Step 6: Generate a 3D mesh grid for surface evaluation\n",
        "x_range = np.linspace(min(x_data), max(x_data), 30)\n",
        "y_range = np.linspace(min(y_data), max(y_data), 30)\n",
        "z_range = np.linspace(min(z_data), max(z_data), 30)\n",
        "\n",
        "X, Y, Z = np.meshgrid(x_range, y_range, z_range)\n",
        "Scores = surface_function((X, Y, Z), a, b, c, d, e, f, g, h, i, j)\n",
        "\n",
        "# 📌 Step 7: Locate the optimal imaging conditions (lowest predicted NIQE score)\n",
        "optimal_idx = np.unravel_index(np.argmin(Scores, axis=None), Scores.shape)\n",
        "optimal_x = X[optimal_idx]\n",
        "optimal_y = Y[optimal_idx]\n",
        "optimal_z = Z[optimal_idx]\n",
        "optimal_score = Scores[optimal_idx]\n",
        "\n",
        "# 📌 Step 8: Print optimal settings\n",
        "print('✅ Optimal Conditions:')\n",
        "print(f'   ap_aperturesize = {optimal_x}')\n",
        "print(f'   ap_wd = {optimal_y}')\n",
        "print(f'   ap_actualkv = {optimal_z}')\n",
        "print(f'   Lowest Score (Best Quality) = {optimal_score}')\n",
        "\n",
        "# 📌 Step 9: Prepare 3D surface plot (fixing one variable for visualization)\n",
        "z_fixed = np.mean(z_data)  # Fix accelerating voltage at its average\n",
        "X_plot, Y_plot = np.meshgrid(x_range, y_range)\n",
        "Z_plot = surface_function(\n",
        "    (X_plot, Y_plot, np.full_like(X_plot, z_fixed)),\n",
        "    a, b, c, d, e, f, g, h, i, j\n",
        ")\n",
        "\n",
        "# 📌 Step 10: Visualize real data and regression surface\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot original data points\n",
        "ax.scatter(x_data, y_data, score_data, color='red', label='Real Data')\n",
        "\n",
        "# Plot fitted regression surface\n",
        "ax.plot_surface(X_plot, Y_plot, Z_plot, cmap='viridis', alpha=0.7)\n",
        "\n",
        "# Add labels and title\n",
        "ax.set_xlabel('ap_aperturesize')\n",
        "ax.set_ylabel('ap_wd')\n",
        "ax.set_zlabel('Score')\n",
        "ax.set_title(f'Surface Regression (Fixed ap_actualkv = {z_fixed:.2f})')\n",
        "\n",
        "# Show legend and plot\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O2OnrNOHXWf6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}